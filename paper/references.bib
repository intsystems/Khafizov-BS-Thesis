@misc{beznosikov2024biasedcompressiondistributedlearning,
      title={On Biased Compression for Distributed Learning}, 
      author={Aleksandr Beznosikov and Samuel Horváth and Peter Richtárik and Mher Safaryan},
      year={2024},
      eprint={2002.12410},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.12410}, 
}
@misc{horvath2022naturalcompressiondistributeddeep,
      title={Natural Compression for Distributed Deep Learning}, 
      author={Samuel Horvath and Chen-Yu Ho and Ludovit Horvath and Atal Narayan Sahu and Marco Canini and Peter Richtarik},
      year={2022},
      eprint={1905.10988},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1905.10988}, 
}
@misc{mishchenko2023distributedlearningcompressedgradient,
      title={Distributed Learning with Compressed Gradient Differences}, 
      author={Konstantin Mishchenko and Eduard Gorbunov and Martin Takáč and Peter Richtárik},
      year={2023},
      eprint={1901.09269},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1901.09269}, 
}
@misc{alistarh2017qsgdcommunicationefficientsgdgradient,
      title={QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding}, 
      author={Dan Alistarh and Demjan Grubic and Jerry Li and Ryota Tomioka and Milan Vojnovic},
      year={2017},
      eprint={1610.02132},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1610.02132}, 
}
@misc{wen2017terngradternarygradientsreduce,
      title={TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning}, 
      author={Wei Wen and Cong Xu and Feng Yan and Chunpeng Wu and Yandan Wang and Yiran Chen and Hai Li},
      year={2017},
      eprint={1705.07878},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1705.07878}, 
}
@misc{bernstein2018signsgdcompressedoptimisationnonconvex,
      title={signSGD: Compressed Optimisation for Non-Convex Problems}, 
      author={Jeremy Bernstein and Yu-Xiang Wang and Kamyar Azizzadenesheli and Anima Anandkumar},
      year={2018},
      eprint={1802.04434},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1802.04434}, 
}
@misc{vogels2020powersgdpracticallowrankgradient,
      title={PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization}, 
      author={Thijs Vogels and Sai Praneeth Karimireddy and Martin Jaggi},
      year={2020},
      eprint={1905.13727},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1905.13727}, 
}
@misc{stich2021errorfeedbackframeworkbetterrates,
      title={The Error-Feedback Framework: Better Rates for SGD with Delayed Gradients and Compressed Communication}, 
      author={Sebastian U. Stich and Sai Praneeth Karimireddy},
      year={2021},
      eprint={1909.05350},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1909.05350}, 
}
@misc{richtárik2021ef21newsimplertheoretically,
      title={EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback}, 
      author={Peter Richtárik and Igor Sokolov and Ilyas Fatkhullin},
      year={2021},
      eprint={2106.05203},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.05203}, 
}