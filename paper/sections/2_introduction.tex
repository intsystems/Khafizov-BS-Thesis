\section{Введение}

    В распределённой оптимизации рассматривается задача минимизации усреднённого функционала
    \begin{equation}
        \min_{x\in\mathbb{R}^d} f(x) = \frac{1}{n}\sum_{i=1}^n f_i(x),
    \end{equation}
    где $f_i(x)$~--- функция потерь на $i$-ом узле сети. Такая постановка характерна как для классических распределённых обучающих кластеров, так и для федеративного обучения (много устройств с разнородными данными) \cite{beznosikov2024biasedcompressiondistributedlearning}. Базовый алгоритм — распределённый градиентный спуск (DGD)~--- выполняет итерации
    \begin{equation}\label{eq:dgd}
        x^{k + 1} = x^k - \eta_k \frac{1}{n}\sum_{i=1}^n \nabla f_i(x^k),
    \end{equation}
    где $\eta_k > 0$~--- шаг обучения. На практике именно передача в сеть полных векторов градиентов $\nabla f_i(x^k)$ становится узким местом (bottleneck) из-за высоких требований к пропускной способности и латентности каналов \cite{beznosikov2024biasedcompressiondistributedlearning}. Особенно это важно при обучении больших языковых моделей (сотни миллионов–миллиард параметров) или в федеративных системах на слабых устройствах.


    В связи с этим предложены методы сжатия градиентов, при которых каждый узел отправляет не полный градиент, а его компрессированную версию $\widetilde{g}_i^k = \mathcal{C}(g_i^k)$, где $g_i^k = \nabla f_i(x^k)$. Оператор $\mathcal{C}:\mathbb{R}^d \to \mathbb{R}^d$ называется оператором сжатия или компрессором. Важными свойствами компрессора являются сохранение <<ориентира>> градиента и контроль ошибки. Обычно вводят понятие несмещённого компрессора $\mathcal{C} \in U(\omega)$, для которого выполняется
    \begin{align*}
        \mathbb{E}[\mathcal{C}(x)] =& x,\\
        \mathbb{E}[\|\mathcal{C}(x) - x\|_2^2] \leq& \omega \|x\|_2^2
    \end{align*}
    для некоторого параметра $\omega \geq 0$ \cite{horvath2022naturalcompressiondistributeddeep}. При $\omega = 0$ компрессор передаёт точный вектор. Анализ показывает, что использование несмещённых операторов $\mathcal{C}$ сохраняет схожесть с обычным SGD, лишь увеличивая дисперсию градиентов \cite{horvath2022naturalcompressiondistributeddeep,mishchenko2023distributedlearningcompressedgradient}. Примеры несмещённых операторов: $\randk$ — случайное прореживание $k$ компонент с масштабированием $d/k$ (формально $\mathcal{C}(x) = (d / k) \sum_{i \in S} x_i e_i$ при случайном выборе $k$ индексов $S$) \cite{beznosikov2024biasedcompressiondistributedlearning}, QSGD (стохастическая квантизация градиента) \cite{alistarh2017qsgdcommunicationefficientsgdgradient}, TernGrad (обрезка до трёх значений) \cite{wen2017terngradternarygradientsreduce}, или натуральное сжатие (случайное округление до степеней двойки), которое даёт малую дисперсию ($\omega=1/8$) \cite{horvath2022naturalcompressiondistributeddeep}. Все они удовлетворяют $\mathbb{E}[\mathcal{C}(x)] = x$ и ограничению дисперсии. Например, оператор $\randk$ принадлежит классу $U(d / k)$ \cite{beznosikov2024biasedcompressiondistributedlearning}, а натуральная компрессия имеет очень низкий коэффициент $\omega=1/8$ \cite{horvath2022naturalcompressiondistributeddeep}, что лишь незначительно замедляет сходимость.

    В то же время широко используются и смещённые компрессоры, у которых $\mathbb{E}[\mathcal{C}(x)] \neq x$. Классическим примером является $\topk$-сжатие, при котором передаются $k$ компонент градиента с наибольшими по модулю значениями, остальные равны нулю \cite{beznosikov2024biasedcompressiondistributedlearning}. Формально
    \begin{equation}
        \mathcal{C}(x) = \sum_{i=d-k+1}^d x_{(i)} e_{(i)},
    \end{equation}
    где $|x_{(1)}| \leq \cdots \leq |x_{(d)}|$ — отсортированные по модулю координаты \cite{beznosikov2024biasedcompressiondistributedlearning}. Оператор $\topk$ является детерминированным и смещённым, он не сохраняет матожидание исходного вектора и, как следствие, прямое применение в DGD приводит к расходимости \cite{beznosikov2024biasedcompressiondistributedlearning}. Среди смещённых схем можно также назвать адаптивную спарсификацию (выбор координаты с вероятностью, пропорциональной её модулю) \cite{beznosikov2024biasedcompressiondistributedlearning} и ранжированное квантование. 

    Кроме прямого отбора компонент, часто применяются методы квантования. Так, в QSGD каждую координату случайно округляют между ближайшими уровнями с учётом нормы вектора \cite{alistarh2017qsgdcommunicationefficientsgdgradient}, что тоже даёт $\mathbb{E}[\mathcal{C}(x)] = x$. В SignSGD передаются лишь знаки координат (1 бит на компоненту) \cite{bernstein2018signsgdcompressedoptimisationnonconvex}. В PowerSGD градиент-матрица аппроксимируется низкоранговой матрицей с помощью двух рандомно инициализированных векторов (метод низкого ранга), что эффективно сжимает сингулярный спектр градиента \cite{vogels2020powersgdpracticallowrankgradient}. Все эти подходы демонстрируют существенную экономию трафика при обучении нейронных сетей.

    Классификация компрессоров на смещённые и несмещённые существенно влияет на анализ сходимости. При использовании несмещённых операторов можно получить стандартые оценки для SGD/DGD с увеличенной дисперсией градиента (например, $O(1/\sqrt{T})$ для SGD) \cite{alistarh2017qsgdcommunicationefficientsgdgradient,horvath2022naturalcompressiondistributeddeep}. Для смещённых компрессоров требуется дополнительный механизм компенсации ошибки, иначе метод может расходиться \cite{beznosikov2024biasedcompressiondistributedlearning}. 


    Рассмотрим и другой подход, часто используемый в распределенном обучении. Компенсация ошибок (Error Feedback, EF)~--- метод, изначально предложенный на практике и проанализированный в последние годы \cite{stich2021errorfeedbackframeworkbetterrates,beznosikov2024biasedcompressiondistributedlearning}. Идея в том, что каждый узел хранит текущую ошибку $e_i^k$ компрессии и <<добавляет>> её к новому градиенту: вычисляется сжатый вектор
    \begin{equation}
        \widetilde{g}_i^k = \mathcal{C}\left(g_i^k + e_i^k\right),
    \end{equation}
    после чего обновляется ошибка
    \begin{equation}
        e_i^{k+1} = g_i^k + e_i^k - \widetilde{g}_i^k.
    \end{equation}
    Таким образом ошибки сжатия накапливаются и в дальнейшем компенсируют смещение. В работах показано, что механизм EF обобщённо гарантирует сходимость алгоритма при произвольных компрессорах \cite{stich2021errorfeedbackframeworkbetterrates,vogels2020powersgdpracticallowrankgradient}. В частности, Stich и Karimireddy (2019) продемонстрировали, что даже при запаздывающих или сильно шумных обновлениях с EF достигаются лучшие известные оценки сходимости \cite{stich2021errorfeedbackframeworkbetterrates}. По сути, метод DCGD с EF использует обновление
    \begin{equation}
        x^{k+1} = x^k - \eta \frac{1}{n}\sum_{i=1}^n \widetilde{g}_i^k
    \end{equation}
    с учётом ошибок $e_i^k$ \cite{vogels2020powersgdpracticallowrankgradient,beznosikov2024biasedcompressiondistributedlearning}. 

    Новые разработки модифицируют классическую схему EF. Так, алгоритм EF21 \cite{richtárik2021ef21newsimplertheoretically} предлагает иной способ хранения <<памяти>> сжатия: вместо вектора ошибки в чистом виде хранится скопленный оценочный градиент $g_i^k$, который обновляется по правилу 
    \begin{equation}
        g_i^{k+1} = g_i^k + \mathcal{C}\left(\nabla f_i(x^{k+1}) - g_i^k\right),
    \end{equation}
    а шаг по $x$ идёт как $x^{k+1} = x^k - \eta\frac{1}{n}\sum_i g_i^k$ \cite{richtárik2021ef21newsimplertheoretically}. Такое переопределение сохраняет выигрыши EF и обеспечивает более простую теорию. Для гладких невыпуклых задач EF21 гарантирует скорость сходимости $\mathcal{O}(1/T)$, что лучше классического $\mathcal{O}(T^{-2/3})$ для EF \cite{richtárik2021ef21newsimplertheoretically}. При выполнении функцией условия Поляка-Лойясевича (PL-condition) EF21 даже даёт линейную сходимость. 

    Стоит отметить и другие алгоритмические схемы. Например, алгоритм DIANA использует сжатие разности градиентов $g_i^k - h_i^k$ с обновлением локальной модели $h_i^k$, что позволяет сохранить несмещённость и добиться линейной сходимости в сильно выпуклом случае \cite{mishchenko2023distributedlearningcompressedgradient}. Хотя DIANA не относится к EF-схемам, она решает схожую задачу компенсации шума сжатия.


    Ставятся следующие вопросы:
    \begin{itemize}
        \item Можно ли предложить новый смещенный оператор сжатия, который будет учитывать важность компонент при их отборе?
        \item Подходят ли старые схемы компенсации ошибки для новых операторов сжатия?
        \item Можно ли модифицировать схемы коррекции ошибки, чтобы получить более быструю сходимость?
        \item Как все новые методы покажут себя на практических задачах по сравнению с уже известными?
    \end{itemize}
    В соответствии с поставленными вопросами, эта работа представляет следующие результаты:
    \begin{itemize}
        \item Предложено новое семейство операторов сжатия $\impk$, которое учитывает важность координат в процессе оптимизации.
        \item Для оператора $\impk$ с выбором множества $Q = [1, 2]^d$ получена теоретическая оценка на скорость сходимости и оценка на количество итераций $\mathcal{O}\left(\frac{L}{\mu} \frac{d}{k} \log\left(\frac{f(x^0) - f^*}{\varepsilon}\right)\right)$ для достижения $\varepsilon$-точности по функции.
        \item Предложен новый механизм компенсации ошибок SCAM, который значительно улучшает скорости сходимости как для новых операторов сжатия, так и для существующих смещённых компрессоров.
        \item Для схемы компенсации SCAM получена теоретическая оценка на скорость сходимости и оценка на количество итераций $\mathcal{O}\left(\frac{L}{\mu} \frac{d}{k} \log\left(\frac{f(x^0) - f^*}{\varepsilon}\right)\right)$ для достижения $\varepsilon$-точности по функции.
        \item Проведены эксперименты на различных архитектурах, которые подтверждают эффективность предложенных методов и их превосходство над существующими решениями в области распределённой оптимизации.
    \end{itemize}