\section{Введение}
В распределённой оптимизации рассматривается задача минимизации усреднённого функционала
\begin{equation}
\min_{x\in\mathbb{R}^d} f(x) = \frac{1}{n}\sum_{i=1}^n f_i(x),
\end{equation}
где $f_i(x)$ – функция потерь на $i$-ом узле сети. Такая постановка характерна как для классических распределённых обучающих кластеров, так и для федеративного обучения (много устройств с разнородными данными) \cite{Beznosikov2023}. Базовый алгоритм — распределённый градиентный спуск (DGD) — выполняет итерации
\begin{equation}
\label{eq:dgd}
x^{k+1} = x^k - \eta_k \frac{1}{n}\sum_{i=1}^n \nabla f_i(x^k),
\end{equation}
где $\eta_k>0$ — шаг обучения. На практике именно передача в сеть полных векторов градиентов $\nabla f_i(x^k)$ становится узким местом (bottleneck) из-за высоких требований к пропускной способности и латентности каналов \cite{Beznosikov2023}. Особенно это важно при обучении больших языковых моделей (сотни миллионов–миллиард параметров) или в федеративных системах на слабых устройствах. 

В связи с этим предложены методы сжатия градиентов, при которых каждый узел отправляет не полный градиент, а его компрессированную версию $\widetilde{g}_i^k = \mathcal{C}(g_i^k)$, где $g_i^k=\nabla f_i(x^k)$. Оператор $\mathcal{C}:\mathbb{R}^d\to\mathbb{R}^d$ называется оператором сжатия или компрессором. Важными свойствами компрессора являются сохранение «ориентира» градиента и контроль ошибки. Обычно вводят понятие несмещённого компрессора $ \mathcal{C}\in U(\zeta)$, для которого выполняется
\begin{align}
\mathbb{E}[\mathcal{C}(x)] &= x,\\
\mathbb{E}\|\mathcal{C}(x)-x\|^2 &\le \omega \|x\|^2
\end{align}
для некоторого параметра $\omega\ge0$ \cite{Horvath2022}. При $\omega=0$ компрессор передаёт точный вектор. Анализ показывает, что использование несмещённых операторов $\mathcal{C}$ сохраняет схожесть с обычным SGD, лишь увеличивая дисперсию градиентов \cite{Horvath2022,Mishchenko2019}. Примеры несмещённых схем: Rand-\(k\) — случайная спарсификация \(k\) компонент с масштабированием \(d/k\) (формально \(\mathcal{C}(x) = (d/k)\sum_{i\in S} x_i e_i\) при случайном выборе \(k\) индексов \(S\)) \cite{Beznosikov2023}, QSGD (стохастическая квантизация градиента) \cite{Alistarh2017}, TernGrad (обрезка до трёх значений) \cite{Wen2017}, или натуральное сжатие (случайное округление до степеней двойки), которое даёт малую дисперсию (\(\omega=1/8\)) \cite{Horvath2022}. Все они удовлетворяют \(\mathbb{E}\mathcal{C}(x)=x\) и ограничению дисперсии. Например, оператор Rand-\(k\) принадлежит классу \(U(d/k)\) \cite{Beznosikov2023}, а натуральная компрессия имеет очень низкий коэффициент \(\omega=1/8\) \cite{Horvath2022}, что лишь незначительно замедляет сходимость.

В то же время широко используются и смещённые компрессоры, у которых \(\mathbb{E}[\mathcal{C}(x)]\neq x\). Классическим примером является Top-\(k\)–сжатие, при котором передаются \(k\) компонент градиента с наибольшими по модулю значениями, остальные равны нулю \cite{Beznosikov2023}. Формально
\begin{equation}
\mathcal{C}(x) = \sum_{i=d-k+1}^d x_{(i)} e_{(i)},
\end{equation}
где \(|x_{(1)}|\le\cdots\le|x_{(d)}|\) — отсортированные по модулю координаты \cite{Beznosikov2023}. Оператор Top-\(k\) является детерминированным и смещённым, он не сохраняет ожидание исходного вектора и, как следствие, прямое применение в DGD приводит к расходимости \cite{Beznosikov2023}. Среди смещённых схем можно также назвать адаптивную спарсификацию (выбор координаты с вероятностью, пропорциональной её модулю) \cite{Beznosikov2023} и ранжированное квантование. 

Кроме прямого отбора компонент, часто применяются методы квантования. Так, в QSGD каждую координату случайно округляют между ближайшими уровнями с учётом нормы вектора \cite{Alistarh2017}, что тоже даёт \(\mathbb{E}[\mathcal{C}(x)] = x\). В SignSGD передаются лишь знаки координат (1 бит на компоненту) \cite{Bernstein2018}. В PowerSGD (Wangni et al. 2018) градиент-матрица аппроксимируется низкоранговой матрицей с помощью двух рандомно инициализированных векторов (метод низкого ранга), что эффективно сжимает сингулярный спектр градиента \cite{Vogels2019}. Все эти подходы демонстрируют существенную экономию трафика при обучении нейронных сетей.

Классификация компрессоров на смещённые и несмещённые существенно влияет на анализ сходимости. При использовании несмещённых операторов можно получить стандартые оценки для SGD/DGD с увеличенной дисперсией градиента (например, \(O(1/\sqrt{T})\) для SGD) \cite{Alistarh2017,Horvath2022}. Для смещённых компрессоров требуется дополнительный механизм компенсации ошибки, иначе метод может расходиться \cite{Beznosikov2023}. 

\subsection{Схемы компенсации ошибок}
    Компенсация ошибок (Error Feedback, EF) — метод, изначально предложенный на практике (Seide et al., 2014) и проанализированный в последние годы \cite{Stich2020,Beznosikov2023}. Идея в том, что каждый узел хранит текущую ошибку \(e_i^k\) компрессии и «добавляет» её к новому градиенту: вычисляется сжатый вектор
    \begin{equation}
    \widetilde{g}_i^k = \mathcal{C}\bigl(g_i^k + e_i^k\bigr),
    \end{equation}
    после чего обновляется ошибка
    \begin{equation}
    e_i^{k+1} = g_i^k + e_i^k - \widetilde{g}_i^k.
    \end{equation}
    Таким образом ошибки сжатия накапливаются и в дальнейшем компенсируют смещение. В работах показано, что механизм EF обобщённо гарантирует сходимость алгоритма при произвольных компрессорах \cite{Stich2020,Vogels2019}. В частности, Stich и Karimireddy (2019) продемонстрировали, что даже при запаздывающих или сильно шумных обновлениях с EF достигаются лучшие известные оценки сходимости \cite{Stich2020}. По сути, метод `Gradient Descent с ошибочным сигналом` использует обновление
    \begin{equation}
    x^{k+1} = x^k - \eta \frac{1}{n}\sum_{i=1}^n \widetilde{g}_i^k
    \end{equation}
    с учётом ошибок \(e_i^k\) \cite{Vogels2019,Beznosikov2023}. 

    Новые разработки модифицируют классическую схему EF. Так, алгоритм EF21 (Richtárik et al., 2021) предлагает иной способ хранения «памяти» сжатия: вместо вектора ошибки в чистом виде хранится скопленный оценочный градиент \(g_i^k\), который обновляется по правилу 
    \begin{equation}
    g_i^{k+1} = g_i^k + \mathcal{C}\bigl(\nabla f_i(x^{k+1}) - g_i^k\bigr),
    \end{equation}
    а шаг по \(x\) идёт как \(x^{k+1}=x^k - \eta\frac{1}{n}\sum_i g_i^k\) \cite{Richtarik2021}. Такое переопределение сохраняет выигрыши EF и обеспечивает более простую теорию. Для гладких невыпуклых задач EF21 гарантирует скорость сходимости \(O(1/T)\), что лучше классического \(O(T^{-2/3})\) для EF \cite{Richtarik2021}. При выполнении функцией условия Поляка-Лойясевича (PL-условия) EF21 даже даёт линейную сходимость. 

    Стоит отметить и другие алгоритмические схемы. Например, алгоритм DIANA (Mishchenko et al., 2019) использует сжатие разности градиентов \(g_i^k - h_i^k\) с обновлением локальной модели \(h_i^k\), что позволяет сохранить несмещённость и добиться линейной сходимости в сильно выпуклом случае \cite{Mishchenko2019}. Хотя DIANA не относится к EF-схемам, он решает схожую задачу компенсации шума сжатия. 

    Приведём краткий обзор основных компрессоров и схем компенсации:
    \begin{itemize}
        \item Rand-\(k\) (случайная спарсификация): передаются \(k\) случайных компонент градиента, масштабированного на \(d/k\); оператор несмещённый (наличие мультипликатора делает \(\mathbb{E}[\widetilde{g}]=g\)) \cite{Beznosikov2023}.
        \item Top-\(k\) (жадная спарсификация): передаются \(k\) наиболее значимых компонент по модулю \cite{Beznosikov2023}. Оператор смещённый; без компенсации даёт ошибку.
        \item SignSGD \cite{Bernstein2018}: передаются только знаки компонент; также смещённый, но при агрегации «голосованием большинства» параметр-сервер получает эффективный градиент \cite{Bernstein2018}.
        \item QSGD \cite{Alistarh2017}: квантизация с рандомным округлением до ограниченного числа бит \cite{Alistarh2017}. Даёт управляемую ошибку дисперсии.
        \item TernGrad \cite{Wen2017}: ограничивает градиенты тремя уровнями (например, \(\{-\|\nabla\|_\infty,0,\|\nabla\|_\infty\}\)) \cite{Wen2017}.
        \item Natural Compression \cite{Horvath2022}: случайное округление до степеней двойки даёт невероятно низкую дисперсию; формально \(\omega=1/8\) \cite{Horvath2022}.
        \item PowerSGD \cite{Vogels2019}: приближает градиент низкоранговым методом Силдера, что эффективно сжимает информацию без сильного искажения \cite{Vogels2019}.
    \end{itemize}

    Во всех перечисленных случаях целевое улучшение — добиться минимального замедления сходимости при существенном сокращении объёма передаваемых данных. Например, доказано, что встроенные компрессоры приводят лишь к добавке ошибки сжатия в квадратичной норме \cite{Beznosikov2023,Horvath2022}, а компенсация ошибок успешно устраняет систематическое смещение \cite{Stich2020,Vogels2019}.

    Наконец, отметим современный контекст применения: такие методы используют в федеративном обучении (сбережение батареи и пропускной способности устройств, см. FedAvg \cite{McMahan2017}, FedProx \cite{Li2020}) и при обучении больших языковых моделей на кластерах (миллиарды параметров требуют обмен сотнями мегабайт за итерацию). Обзор сложностей и открытых проблем федеративного обучения приведён, например, в работе \cite{Kairouz2019}. Применение эффективных компрессоров и схем компенсации ошибок является ключевым для практического ускорения обучения в этих сценариях.

    Таким образом, введение сжатия градиентов и компенсации ошибок в распределённые алгоритмы позволяет значительно сократить коммуникационные затраты без потери сходимости \cite{Richtarik2021,Beznosikov2023}. Ниже будет подробно рассмотрен каждый из описанных методов и обоснована их актуальность в современных приложениях.

% Добавить что выносится на защиту