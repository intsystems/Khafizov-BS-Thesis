\section{Основные результаты}

Результаты работы разделяются на две ветви исследований: изобретение новых операторов сжатия и модификация методов оптимизации с помощью механизмов компенсации ошибки.

\subsection{Использование механизма важности для операторов сжатия}

    В этом разделе мы предлагаем использовать механизм важности для операторов сжатия, который позволяет более эффективно отбирать компоненты градиента при пересылке. Введем формально вектор важности компонент.

    \begin{definition}
        Пусть $f: \R^d \to \R$~--- непрерывно дифференцируемая функция. Вектором важности $w$ в точке $x$ назовем решение задачи минимизации
        \begin{equation}\label{eq:importance_def}
            \argmin_{w \in Q} f(x - \gamma w \odot \nabla f(x)),
        \end{equation}
        где $Q \subset \R^d$~--- ограниченное множество, $\gamma \in \R, \gamma > 0$~--- размер шага.
    \end{definition}
    Трактовать это определение можно следующим образом: число $w_i$ показывает, насколько приоритетной для спуска является компонента $i$. Если $w_i > w_j$ для некоторых $i \neq j$, то вдоль компоненты $i$ можно сделать шаг больше, чем вдоль компоненты $j$. При этом про убывание абсолютного значения целевой функции $f$, исходя только из вектора важности, говорить нельзя, поскольку важную долю информации о функции содержит градиент.

    В качестве ограниченного множества $Q$ удобно выбирать множество, содержащее вектор из единиц $(1, \dots, 1) \in \R^d$, поскольку эта точка соответствует стандартному шагу градиентного спуска. Мы далее будем рассматривать два варианта для $Q$:
    \begin{itemize}
        \item $\Delta_{d - 1} = \{x \in \R^d ~|~ \|x\|_1 = d, x_i \geq 0, i = \overline{1, d}\}$~--- симплекс размерности $d - 1$;
        \item $[a, b]^d$~--- куб со сторонами длиной $b - a$.
    \end{itemize}
    Для решения подзадачи \eqref{eq:importance_def} на описанных множествах будем использовать:
    \begin{itemize}
        \item Метод зеркального спуска на симплексе, в качестве дивергенции Брэгмэна выбрана KL-дивергенция;
        \item Градиентный спуск с проекцией для куба: делаем градиентный шаг, проецируем на куб $[a, b]^d$.
    \end{itemize}
    Выбор методов обусловлен наличием аналитической формулы итерации и простотой в имплементации в коде. Сформулируем утверждение о том, как выглядит итерация каждого из выбранных методов.
    \begin{proposition}
        Пусть решается задача поиска вектора важности \eqref{eq:importance_def} для функции $f: \R^d \to \R$ в точке $x \in \R^d$ с $\gamma > 0$. Значением шага для решения задачи выбрано $\eta > 0$. Тогда:
        \begin{itemize}
            \item при выборе $Q = \Delta_{d - 1}$ итерация метода зеркального спуска имеет вид:
            \begin{equation}\label{eq:mirror_descent_step}
                x^{k + 1} = x^k.
            \end{equation}
            \item при выборе $Q = [a, b]^d$ итерация метода зеркального спуска имеет вид:
            \begin{equation}\label{eq:proj_gradient_descent_step}
                x^{k + 1} = \max(a, \min(b, x^k - \eta \cdot \gamma \nabla f(x^k))),
            \end{equation}
            где операции $\min$ и $\max$ выполняются поэлементно.
        \end{itemize}
    \end{proposition}
    Теперь, когда мы определили, что такое вектор важности, и получили способ эффективно его находить, перейдем к компрессорам, использующим нововведенный механизм. Их можно построить великое множество, поэтому объединим их в семейство.
    \begin{definition}
        Пусть $f$~--- целевая непрерывно дифференцируемая функция, $Q \subset \R^d$~--- ограниченное множество, $\gamma > 0$~--- размер шага. Тогда будем говорить, что оператор сжатия $\mathcal{C}$ принадлежит семейству $\impk$, если он задействует механизм важности при сжатии вектора градиента:
        \begin{equation}
            \cC(\nabla f(x)) = \cC(\nabla f(x), w).
        \end{equation}
    \end{definition}
    Приведем несколько примеров операторов из семейства важностных:
    \begin{itemize}
        \item \textbf{Важностное прореживание}.\\
        Выбираем $k$ компонент с наибольшими значениями важности, то есть,
        \begin{equation}
            \cC(\nabla f(x)) = \sum_{i=1}^k \nabla_{(i)} f(x) e_{(i)},
        \end{equation}
        где $e_i$~--- базисные векторы, а порядок компонент определяется по убыванию значений важности $w_{(1)} \geq w_{(2)} \geq \dots \geq w_{(d)}$.
        \item \textbf{Рандомизированное важностное прореживание}\\
        Выбираем случайные $k$ компонент 
        \begin{equation}
            \cC(\nabla f(x)) = \sum_{i \in S} \nabla_i f(x) e_i,
        \end{equation}
        где $S$~--- множество индексов, выбранных случайно с вероятностью, пропорциональной значениям важности.
        \item \textbf{Важностное прореживание с перевзвешиванием}\\
        Выбираем $k$ компонент с наибольшими значениями $|w_i \cdot \nabla_i f(x)|$, передаем их с весами $w_i$, то есть,
        \begin{equation}
            \cC(\nabla f(x)) = \sum_{i=1}^k w_{(i)} \nabla_{(i)} f(x) e_{(i)},
        \end{equation}
        где $e_i$~--- базисные векторы, а порядок компонент определяется по убыванию значений важности $|w_{(1)} \nabla_{(1)} f(x)| \geq \dots \geq |w_{(d)} \nabla_{(d)} f(x)|$.
    \end{itemize}
    Более подробно будем изучать последний вид компрессоров, поскольку он учитывает и важность компонент, и абсолютное значение градиента, что позволяет делать более оптимальные шаги. Действительно, мы спускаемся вдоль $i$-го параметра модели, в первом приближении убыль функции составит $\nabla_i f(x) \cdot \Delta$, где $\Delta \in \R$~--- изменение значения параметра $x_i$. Таким образом, чем больше $|w_i \cdot \nabla_i f(x)|$, тем большую убыль целевой функции мы получим, сделав шаг $\Delta = w_i \cdot \nabla_i f(x)$ вдоль компоненты $i$. И тогда наш оператор сжатия выбирает наиболее оптимальные для спуска компоненты.

    Запишем метод распределенного градиентного спуска с применением важностного компрессора в случае одного и нескольких устройств.
    \begin{algorithm}[ht]
        \caption{DCGD с важностным компрессором (Одно устройство)}
        \label{alg:dcgd_single}
        \begin{algorithmic}
            \STATE {\bf Ввод:} стартовая точка $x^0$, шаг обучения $\gamma$, количество итераций $T$.
            \FOR{$t = 0, 1, \ldots, T - 1$}
                \STATE $g^t = \nabla f(x^t)$
                \STATE $w^t = \argmin_{w \in Q} f(x^t - \gamma w \odot g^t)$
                \STATE $\tilde{g}^t = \cC(g^t, w^t)$
                \STATE $x^{t+1} = x^t - \gamma \tilde{g}^t$
            \ENDFOR
            \STATE {\bf Вывод:} $x^k$
        \end{algorithmic}
    \end{algorithm}

    \begin{algorithm}[h]
        \caption{DCGD с важностным компрессором (Несколько устройств)}
        \label{alg:dcgd_multi}
        \begin{algorithmic}[1]
            \STATE {\bf Ввод:} стартовая точка $x^0$, шаг обучения $\gamma$, количество итераций $T$, количество устройств $n$.
            \FOR{$t = 0, 1, \ldots, T - 1$}
                \STATE Сервер транслирует $x^t$ на все устройства
                \FOR{на каждом устройстве $i = 1, \ldots, N$}
                    \STATE $g_i^t = \nabla f_i(x^t)$
                    \STATE $w_i^t = \argmin_{w \in Q} f_i(x^t - \gamma w \odot g_i^t)$
                    \STATE $\tilde{g}_i^t = \cC(g_i^t, w_i^t)$
                \ENDFOR
                \STATE $x^{t+1} = x^t - \gamma \frac{1}{n} \sum\limits_{i=1}^n \tilde{g}_i^t$
            \ENDFOR
            \STATE {\bf Вывод:} $x^k$
        \end{algorithmic}
    \end{algorithm}

    Докажем теоретические оценки сходимости для случая одного устройства с $Q = [1, 2]^d$ и важностным компрессором, использующим перевзвешивание компонент.

    \begin{theorem}\label{th:dcgd_single}
        Пусть задача \eqref{eq:problem_statement} решается с помощью DCGD с оператором сжатия $\impk$ (см. алгоритм \ref{alg:dcgd_single}). В качестве множества $Q$ выбран куб $[1, 2]^d$. Тогда для любого $t \geq 1$ выполняется следующее неравенство:
        \begin{equation}
            f(x^{t + 1}) \leq f(x^t) - \gamma \left(1 - \frac{L \gamma}{2}\right) \|\tilde{g}^t\|_2^2,
        \end{equation}
        где $x^0 \in \R^d$~--- стартовая точка, $L$~--- константа гладкости функции $f$, $\gamma$~--- шаг обучения.
    \end{theorem}
    Доказательство приведено в приложении \ref{app:dcgd_single_proof}.

    \begin{lemma}\label{lem:strconv_inequality}
        Пусть $f$~--- $\mu$-сильно выпуклая функция. Тогда выполняется следующее неравенство:
        \begin{equation}\label{eq:strconv_inequality}
            \|\nabla f(x)\|_2^2 \geq 2\mu (f(x) - f^*), \quad \forall x \in \R^d,
        \end{equation}
    \end{lemma}
    Доказательство приведено в приложении \ref{app:strconv_inequality_proof}.

    Далее продолжая анализ сходимости, можем получить теорему о сходимости по функции.

    \begin{theorem}\label{th:dcgd_single_convergence}
        Пусть выполняются условия теоремы \ref{th:dcgd_single}. Тогда для любого $T \geq 1$ выполняется следующее неравенство:
        \begin{equation}
            f(x^T) - f^* \leq \left(1 - 2\mu \gamma \left(\frac{k}{d} - 2 L \gamma\right)\right)^T (f(x^0) - f^*).
        \end{equation}
    \end{theorem}
    Доказательство приведено в приложении \ref{app:dcgd_single_convergence_proof}.

    Наконец, получим оценку на количество итераций, необходимое для достижения заданной точности.
    \begin{corollary}\label{cor:dcgd_single_iterations}
        Пусть выполняются условия теоремы \ref{th:dcgd_single}, значение шага выбрано $\gamma = \frac{k}{4dL}$. Тогда для достижения точности $\varepsilon > 0$ по функци требуется
        \begin{equation}
            T \geq \frac{4L}{\mu} \left(\frac{d}{k}\right)^2 \log\left(\frac{f(x^0) - f^*}{\varepsilon}\right)
        \end{equation}
        итераций.
    \end{corollary}
    Доказательство приведено в приложении \ref{app:dcgd_single_iterations_proof}.

    Полученная оценка скорости сходимости является линейной, однако фактор $(\frac{d}{k})^2$ является достаточно большим.

\subsection{Механизм компенсации ошибок для прореживающих операторов сжатия}
    В этом разделе речь пойдет про новый механизм компенсации ошибки для решения задач федеративного обучения \eqref{eq:problem_statement} с использованием прореживающих операторов сжатия. Имеющиеся на данный момент методы работы с ошибкой от компрессии (EF, EF21) обладают инерцией, даже если она в исходный метод не заложена. В случае Error Feedback это выражается накоплением ошибки с прошлых итераций и пересылке сжатого вектора градиента с ошибкой. В случае EF21 это проявляется в том, что вектор $g$, который известен и серверу, и устройству, также обновляется с задержкой, и при больших долях сжатия большинство компонент в нем сильно устаревшие. Предлагаемая схема SCAM (Sparsification with Compensation and Aggregation Method) лишена этих недостатков. Для простоты начнем со случая одного устройства.

    \begin{algorithm}[H]
        \caption{SCAM (Одно устройство)}
        \label{alg:scam_single}
        \begin{algorithmic}
            \STATE {\bf Ввод:} стартовая точка $x^0$, шаг обучения $\gamma$, количество итераций $T$, начальная ошибка $\varepsilon^0 = 0$.
            \FOR{$t = 0, 1, \ldots, T - 1$}
                \STATE $g^t = \nabla f(x^t)$
                \STATE $c = \cC^t (\varepsilon^t + g^t)$
                \STATE $\tilde{g}^t = \cC^t \left(\sum\limits_{i=1}^{d} I\{c_i \neq 0\} g^t_i e_i\right)$
                \STATE $\varepsilon^t = \varepsilon^{t-1} + g^t - \tilde{g}^t$
                \STATE $x^{t+1} = x^t - \gamma \tilde{g}^t$
            \ENDFOR
            \STATE {\bf Вывод:} $x^k$
        \end{algorithmic}
    \end{algorithm}

    Как мы видим, схема во многом похожа на стандартный Error Feedback: также накапливается ошибка, выбор компонент происходит с учетом вектора $\varepsilon + g$. Далее идут отличия~--- передается не сжатый вектор градиента с накопленной ошибкой, а чистый градиент, к которому был применен оператор компрессии после выбора компонент на $\varepsilon + g$. Это позволяет совместить два важных свойства. Во-первых, из-за накопления и учета ошибки с прошлых итераций веса, требуемые обновления, но имеющие маленькие значения градиентов, отбираются. Во-вторых, из-за сжатия и пересылки чистого градиента метод делает шаги более близкие к истинному направлению убывания функции.

    Обобщим схему на случай нескольких устройств. Добавится внутренний цикл по устройствам, каждое устройство делает тот же шаг, что и в схеме \ref{alg:scam_single} для своей целевой функции $f_i$, после чего на сервере происходит усреднение всех пересланных градиентов и обновление весов модели.

    \begin{algorithm}[H]
        \caption{SCAM (Несколько устройств)}
        \label{alg:scam_multi}
        \begin{algorithmic}[1]
            \STATE {\bf Ввод:} стартовая точка $x^0$, шаг обучения $\gamma$, количество итераций $T$, количество устройств $n$, начальные ошибки $\varepsilon_i^0 = 0, i = \overline{1, n}$.
            \FOR{$t = 0, 1, \ldots, T - 1$}
                \STATE Сервер транслирует $x^t$ на все устройства
                \FOR{на каждом устройстве $i = 1, \ldots, N$}
                    \STATE $g_i^t = \nabla f_i(x^t)$
                    \STATE $c = \cC^t (\varepsilon_i^t + g_i^t)$
                    \STATE $\tilde{g}_i^t = \cC^t \left(\sum\limits_{j=1}^{d} I\{c_j \neq 0\} (g_i^t)_j e_j\right)$
                    \STATE $\varepsilon_i^{t + 1} = \varepsilon_i^{t} + g_i^t - \tilde{g}_i^t$
                \ENDFOR
                \STATE $x^{t+1} = x^t - \gamma \frac{1}{n} \sum\limits_{i = 1}^n \tilde{g}_i^t$
            \ENDFOR
            \STATE {\bf Вывод:} $x^k$
        \end{algorithmic}
    \end{algorithm}

    Можно заметить, что для несмещенных или рандомизированнх компрессоров метод подходит не так хорошо. В самом деле, если рассмотреть классического представителя класса несмещенных компрессоров $\randk$, то он будет отбираать случаные координаты на этапе сжатия $\varepsilon + g$, и после наложения бинарной маски, снова будет выбирать случайные компоненты градиента. Это не имеет особого смысла, ведь не учитывается ни ошибка, ни значения градиента.

    Однако, для смещенных операторов сжатия эта схема представляет больший интерес. Например, при использовании с $\topk$ или $\impk$ все описанные ранее преимущества сохраняются. Далее проведем теоретический анализ схемы SCAM в случае одного устройства с компрессором $\topk$.

    \begin{theorem}\label{th:scam_single}
        Пусть задача \eqref{eq:problem_statement} решается с помощью схемы SCAM (см. алгоритм \ref{alg:scam_single}) с оператором сжатия $\topk$ и начальной ошибкой $\varepsilon^0 = 0$. Тогда для любого $t \geq 1$ выполняется следующее неравенство:
        \begin{equation}
            f(x^{t + 1}) \leq f(x^t) - \gamma \left(1 - \frac{L \gamma}{2}\right) \|\tilde{g}^t\|_2^2,
        \end{equation}
        где $x^0 \in \R^d$~--- стартовая точка, $L$~--- константа гладкости функции $f$, $\gamma$~--- шаг обучения.
    \end{theorem}
    Доказательство приведено в приложении \ref{app:scam_single_proof}.
    
    Для дальнейшего анализа сделаем предположение о частичном сохранении нормы градиента при сжатии оператором $\topk$.
    \begin{proposition}\label{prop:scam_single_partial_norm}
        Будем считать, что в схеме SCAM (см. алгоритм \ref{alg:scam_single}) оператор сжатия $\topk$ частично сохраняет норму градиента, то есть для любого $t \geq 0$ выполняется следующее неравенство:
        \begin{equation}
            \|\tilde{g}^t\|_2^2 \geq \delta \|g^t\|_2^2,
        \end{equation}
        где $\delta > 0$~--- константа.
    \end{proposition}
    Это предположение вполне разумно и по сути оно утверждает, что мы не отбираем только слишком маленькие компоненты. В самом деле, если мы отбираем только маленькие по значениям компоненты, то по большим накапливается ошибка, поэтому на следующей итерации будут выбраны оны. На практике же этот процесс более комплексный и такой ситуации, что отобраны только маленькие значения, не возникает. Параметр $\delta$ можно оценить как $\delta \approx \frac{k}{d}$, исходя из равномерного выбора координат. При этом предположении продолжим анализ.
    \begin{theorem}\label{th:scam_single_convergence}
        Пусть выполняются условия теоремы \ref{th:scam_single} и предположение \ref{prop:scam_single_partial_norm} о частичном сохранении нормы градиента. Тогда для любого $T \geq 1$ выполняется следующее неравенство:
        \begin{equation}
            f(x^T) - f^* \leq (1 - \mu \gamma (2 - L \gamma) \delta)^T (f(x^0) - f^*).
        \end{equation}
    \end{theorem}
    Доказательство приведено в приложении \ref{app:scam_single_convergence_proof}.

    Наконец, сделаем оценку на количество итераций для достижения наперед заданной точности $\varepsilon > 0$.
    \begin{corollary}\label{cor:scam_single_iterations}
        Пусть выполняются условия теоремы \ref{th:scam_single} и предположение \ref{prop:scam_single_partial_norm} о частичном сохранении нормы градиента, значение шага выбрано $\gamma = \frac{1}{L}$. Тогда для достижения точности $\varepsilon > 0$ требуется
        \begin{equation}
            T \geq \frac{L}{\mu \delta} \log\left(\frac{f(x^0) - f^*}{\varepsilon}\right)
        \end{equation}
        итераций.
    \end{corollary}
    Доказательство приведено в приложении \ref{app:scam_single_iterations_proof}.

    Для оценки $\delta \approx \frac{k}{d}$, где $k$~--- количество отбираемых компонент, получаем, что для достижения точности $\varepsilon$ требуется
    \begin{equation}
        T \simeq \frac{L d}{\mu k} \log\left(\frac{f(x^0) - f^*}{\varepsilon}\right).
    \end{equation}
